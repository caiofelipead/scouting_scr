"""
Baixar Fotos do Transfermarkt - Vers√£o com Scraping
Busca a URL correta da foto na p√°gina do jogador
"""

import os
import re
import time

import pandas as pd
import requests
from bs4 import BeautifulSoup

from src.database.database import ScoutingDatabase


def extrair_id_da_url(tm_value):
    """
    Extrai o ID num√©rico do Transfermarkt de uma URL ou string

    Exemplos:
    - https://www.transfermarkt.com.br/adriano/profil/spieler/1046580 -> 1046580
    - 1046580 -> 1046580
    """
    if pd.isna(tm_value) or str(tm_value).strip() == "":
        return None

    tm_str = str(tm_value).strip()

    # Tentar extrair ID num√©rico da URL
    match = re.search(r"/spieler/(\d+)", tm_str)
    if match:
        return match.group(1)

    # Se n√£o encontrar na URL, verificar se j√° √© um ID num√©rico
    if tm_str.isdigit():
        return tm_str

    return None


def extrair_url_foto_da_pagina(tm_id):
    """
    Acessa a p√°gina do jogador e extrai a URL completa da foto
    """
    url_pagina = f"https://www.transfermarkt.com.br/player/profil/spieler/{tm_id}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url_pagina, headers=headers, timeout=15)

        if response.status_code != 200:
            return None, f"Status {response.status_code}"

        soup = BeautifulSoup(response.content, "html.parser")

        # Procurar pela tag img com a foto do jogador
        # Padr√£o: <img src='https://img.a.transfermarkt.technology/portrait/big/68290-1692601435.jpg?lm=1' ...>

        # M√©todo 1: Buscar no modal da foto
        modal_img = soup.find("img", {"src": re.compile(r"portrait/big/.*\.jpg")})
        if modal_img and modal_img.get("src"):
            url_foto = modal_img["src"]
            # Remover par√¢metros de query (?lm=1)
            url_foto = url_foto.split("?")[0]
            return url_foto, "OK"

        # M√©todo 2: Buscar em data-src
        modal_img = soup.find("img", {"data-src": re.compile(r"portrait/big/.*\.jpg")})
        if modal_img and modal_img.get("data-src"):
            url_foto = modal_img["data-src"]
            url_foto = url_foto.split("?")[0]
            return url_foto, "OK"

        # M√©todo 3: Buscar qualquer img com portrait/big
        for img in soup.find_all("img"):
            src = img.get("src", "") or img.get("data-src", "")
            if "portrait/big" in src and ".jpg" in src:
                url_foto = src.split("?")[0]
                return url_foto, "OK"

        return None, "URL n√£o encontrada no HTML"

    except requests.Timeout:
        return None, "Timeout"
    except Exception as e:
        return None, str(e)


def baixar_foto_com_scraping(tm_value, id_jogador, nome_jogador):
    """
    Baixa foto fazendo scraping da p√°gina do jogador
    """
    # Extrair ID num√©rico da URL ou string
    tm_id = extrair_id_da_url(tm_value)

    if not tm_id:
        return False, "ID inv√°lido"

    # Extrair URL da foto da p√°gina
    url_foto, motivo = extrair_url_foto_da_pagina(tm_id)

    if not url_foto:
        return False, motivo

    # Baixar a foto
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

        response = requests.get(url_foto, headers=headers, timeout=10)

        if response.status_code == 200 and len(response.content) > 1000:
            foto_path = f"fotos/{id_jogador}.jpg"
            with open(foto_path, "wb") as f:
                f.write(response.content)
            return True, "OK"
        else:
            return False, f"Status {response.status_code}"

    except Exception as e:
        return False, str(e)


def baixar_todas_fotos_scraping(delay=2.0, max_jogadores=None):
    """
    Baixa fotos de todos os jogadores usando scraping
    """
    print("\n" + "=" * 60)
    print("üì∏ DOWNLOAD DE FOTOS - M√âTODO SCRAPING")
    print("=" * 60)

    # Criar pasta
    os.makedirs("fotos", exist_ok=True)

    # Conectar ao banco
    db = ScoutingDatabase()
    conn = db.connect()

    # Buscar jogadores com TM ID
    query = """
    SELECT id_jogador, nome, transfermarkt_id 
    FROM jogadores 
    WHERE transfermarkt_id IS NOT NULL AND transfermarkt_id != ''
    """

    if max_jogadores:
        query += f" LIMIT {max_jogadores}"

    jogadores = pd.read_sql_query(query, conn)
    conn.close()

    total = len(jogadores)

    if total == 0:
        print("\n‚ùå Nenhum jogador com Transfermarkt ID encontrado!")
        print("\nüí° SOLU√á√ÉO:")
        print("   1. Abra sua planilha do Google Sheets")
        print("   2. Adicione coluna 'TM' (ou similar)")
        print("   3. Preencha com IDs do Transfermarkt")
        print("   4. Execute: python import_data.py")
        print("   5. Execute este script novamente\n")
        return

    print(f"\nüìä {total} jogadores com Transfermarkt ID")
    print(f"‚è±Ô∏è  Delay entre requisi√ß√µes: {delay}s")
    print(f"‚è±Ô∏è  Tempo estimado: {int(total * delay / 60)} minutos")
    print(f"\n‚ö†Ô∏è  IMPORTANTE:")
    print(f"   - Este m√©todo faz scraping das p√°ginas")
    print(f"   - √â mais lento mas mais confi√°vel")
    print(f"   - Respeita rate limiting do site")

    resposta = input("\nPressione ENTER para come√ßar (ou Ctrl+C para cancelar)...")

    sucessos = 0
    falhas = 0
    erros = {}

    print("\nüîÑ Baixando fotos...\n")

    for idx, (_, jogador) in enumerate(jogadores.iterrows(), 1):
        id_jog = jogador["id_jogador"]
        nome = jogador["nome"]
        tm_value = jogador["transfermarkt_id"]

        # Extrair ID para exibi√ß√£o
        tm_id = extrair_id_da_url(tm_value)
        tm_display = tm_id if tm_id else tm_value

        print(f"[{idx}/{total}] {nome} (TM: {tm_display})...", end=" ", flush=True)

        sucesso, motivo = baixar_foto_com_scraping(tm_value, id_jog, nome)

        if sucesso:
            print("‚úÖ")
            sucessos += 1
        else:
            print(f"‚ùå ({motivo})")
            falhas += 1
            erros[motivo] = erros.get(motivo, 0) + 1

        # Delay para n√£o sobrecarregar
        if idx < total:
            time.sleep(delay)

    # Resumo
    print("\n" + "=" * 60)
    print("üìä RESUMO")
    print("=" * 60)
    print(f"‚úÖ Sucessos: {sucessos}/{total} ({sucessos / total * 100:.1f}%)")
    print(f"‚ùå Falhas: {falhas}/{total} ({falhas / total * 100:.1f}%)")

    if erros:
        print("\n‚ùå Motivos das falhas:")
        for motivo, qtd in sorted(erros.items(), key=lambda x: -x[1]):
            print(f"   - {motivo}: {qtd}")

    if sucessos > 0:
        print(f"\n‚úÖ {sucessos} fotos salvas em: fotos/")

    print("=" * 60 + "\n")


def testar_um_jogador(tm_value):
    """
    Testa o scraping para um jogador espec√≠fico
    """
    print("\n" + "=" * 60)
    print("üß™ TESTE DE SCRAPING - UM JOGADOR")
    print("=" * 60)

    # Extrair ID num√©rico
    tm_id = extrair_id_da_url(tm_value)

    if not tm_id:
        print(f"\n‚ùå N√£o foi poss√≠vel extrair ID de: {tm_value}\n")
        return False

    print(f"\nüìã Transfermarkt ID extra√≠do: {tm_id}")
    print(
        f"üåê URL da p√°gina: https://www.transfermarkt.com.br/player/profil/spieler/{tm_id}\n"
    )

    print("1Ô∏è‚É£ Acessando p√°gina do jogador...")
    url_foto, motivo = extrair_url_foto_da_pagina(tm_id)

    if not url_foto:
        print(f"   ‚ùå Falha: {motivo}\n")
        return False

    print(f"   ‚úÖ URL encontrada!")
    print(f"   üì∏ {url_foto}\n")

    print("2Ô∏è‚É£ Baixando foto...")

    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url_foto, headers=headers, timeout=10)

        if response.status_code == 200:
            tamanho = len(response.content)
            print(f"   ‚úÖ Baixado! Tamanho: {tamanho:,} bytes")

            # Salvar temporariamente
            os.makedirs("fotos", exist_ok=True)
            with open("fotos/teste.jpg", "wb") as f:
                f.write(response.content)
            print(f"   üíæ Salvo em: fotos/teste.jpg\n")

            return True
        else:
            print(f"   ‚ùå Status: {response.status_code}\n")
            return False

    except Exception as e:
        print(f"   ‚ùå Erro: {e}\n")
        return False


def menu_principal():
    """Menu interativo"""
    print("\n" + "=" * 60)
    print("üì∏ BAIXAR FOTOS - M√âTODO SCRAPING")
    print("=" * 60)
    print("\n1 - Testar com Neymar (TM ID: 68290)")
    print("2 - Testar com outro jogador (digite o TM ID ou URL)")
    print("3 - Baixar primeiras 5 fotos (teste r√°pido)")
    print("4 - Baixar primeiras 20 fotos (teste m√©dio)")
    print("5 - Baixar TODAS as fotos (modo lento - 2s delay)")
    print("6 - Baixar TODAS as fotos (modo normal - 1.5s delay)")
    print("0 - Sair")
    print("=" * 60)

    opcao = input("\nEscolha uma op√ß√£o: ").strip()

    if opcao == "1":
        testar_um_jogador("68290")

    elif opcao == "2":
        tm_input = input("\nDigite o Transfermarkt ID ou URL: ").strip()
        if tm_input:
            testar_um_jogador(tm_input)

    elif opcao == "3":
        baixar_todas_fotos_scraping(delay=2.0, max_jogadores=5)

    elif opcao == "4":
        baixar_todas_fotos_scraping(delay=2.0, max_jogadores=20)

    elif opcao == "5":
        baixar_todas_fotos_scraping(delay=2.0)

    elif opcao == "6":
        baixar_todas_fotos_scraping(delay=1.5)

    elif opcao == "0":
        print("\nüëã At√© logo!\n")
        return False

    else:
        print("\n‚ùå Op√ß√£o inv√°lida!\n")

    return True


if __name__ == "__main__":
    try:
        continuar = True

        while continuar:
            continuar = menu_principal()

            if continuar:
                input("\n\nPressione ENTER para voltar ao menu...")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio.\n")
    except Exception as e:
        print(f"\n\n‚ùå Erro: {e}\n")
        import traceback

        traceback.print_exc()
